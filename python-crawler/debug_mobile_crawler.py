#!/usr/bin/env python3
"""
ëª¨ë°”ì¼ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ í¬ë¡¤ëŸ¬ ë””ë²„ê¹… ë„êµ¬
ì‹¤ì œ HTML êµ¬ì¡°ì™€ ì„ íƒìë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
"""
import requests
import urllib.parse
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime

class MobileCrawlerDebugger:
    def __init__(self):
        # ëª¨ë°”ì¼ í—¤ë” ì„¤ì •
        self.headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Referer": "https://m.place.naver.com/",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }

    def debug_search(self, keyword):
        """í‚¤ì›Œë“œ ê²€ìƒ‰ ë””ë²„ê¹…"""
        print(f"ğŸ” ë””ë²„ê¹… ì‹œì‘: '{keyword}'")
        print("=" * 60)
        
        # 1. URL ìƒì„± ë° ìš”ì²­
        encoded_keyword = urllib.parse.quote(keyword)
        url = f"https://m.place.naver.com/search?query={encoded_keyword}"
        
        print(f"ğŸ“ ê²€ìƒ‰ URL: {url}")
        print(f"ğŸ“± User-Agent: {self.headers['User-Agent'][:50]}...")
        
        try:
            session = requests.Session()
            session.headers.update(self.headers)
            
            response = session.get(url, timeout=15)
            print(f"ğŸ“Š ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            print(f"ğŸ“Š ì‘ë‹µ í¬ê¸°: {len(response.text)} bytes")
            
            if response.status_code != 200:
                print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                return None
                
            # 2. HTML êµ¬ì¡° ë¶„ì„
            soup = BeautifulSoup(response.text, "html.parser")
            
            # HTML íŒŒì¼ë¡œ ì €ì¥ (ë¶„ì„ìš©)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"debug_mobile_{keyword.replace(' ', '_')}_{timestamp}.html"
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(response.text)
            print(f"ğŸ’¾ HTML ì €ì¥ë¨: {filename}")
            
            # 3. ê°€ëŠ¥í•œ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            print("\nğŸ” ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ë¶„ì„:")
            list_containers = [
                soup.find_all("ul"),
                soup.find_all("div", class_=lambda x: x and "list" in x.lower()),
                soup.find_all("div", class_=lambda x: x and "place" in x.lower()),
                soup.find_all("div", role="list"),
                soup.find_all("section"),
            ]
            
            for container_type, containers in zip(
                ["ul íƒœê·¸", "list í´ë˜ìŠ¤", "place í´ë˜ìŠ¤", "role=list", "section íƒœê·¸"], 
                list_containers
            ):
                if containers:
                    print(f"  ğŸ“‹ {container_type}: {len(containers)}ê°œ ë°œê²¬")
                    for i, container in enumerate(containers[:3]):  # ì²˜ìŒ 3ê°œë§Œ
                        children = container.find_all(["li", "div"], recursive=False)
                        print(f"    â””â”€ {i+1}ë²ˆì§¸: {len(children)}ê°œ ìì‹ ìš”ì†Œ")
            
            # 4. ëª¨ë“  li íƒœê·¸ ë¶„ì„
            all_lis = soup.find_all("li")
            print(f"\nğŸ“‹ ì „ì²´ li íƒœê·¸: {len(all_lis)}ê°œ")
            
            # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” lië§Œ í•„í„°ë§
            text_lis = [li for li in all_lis if li.get_text().strip()]
            print(f"ğŸ“‹ í…ìŠ¤íŠ¸ í¬í•¨ li: {len(text_lis)}ê°œ")
            
            # 5. ìƒìœ„ 10ê°œ li íƒœê·¸ ë‚´ìš© ë¶„ì„
            print(f"\nğŸ” ìƒìœ„ {min(10, len(text_lis))}ê°œ li íƒœê·¸ ë¶„ì„:")
            for i, li in enumerate(text_lis[:10]):
                text = li.get_text().strip()[:50]  # ì²˜ìŒ 50ê¸€ìë§Œ
                classes = li.get('class', [])
                print(f"  {i+1:2d}. í´ë˜ìŠ¤: {classes}")
                print(f"      í…ìŠ¤íŠ¸: {text}...")
                
                # ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
                links = li.find_all('a')
                if links:
                    for link in links[:2]:  # ì²˜ìŒ 2ê°œë§Œ
                        href = link.get('href', '')
                        if 'place' in href:
                            print(f"      ğŸ“ í”Œë ˆì´ìŠ¤ ë§í¬: {href[:50]}...")
                print()
            
            # 6. íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ ìš”ì†Œ ì°¾ê¸°
            print(f"ğŸ” '{keyword}' ê´€ë ¨ ìš”ì†Œ ê²€ìƒ‰:")
            keyword_elements = soup.find_all(text=lambda text: text and keyword in text)
            print(f"  ğŸ“ í‚¤ì›Œë“œ í¬í•¨ í…ìŠ¤íŠ¸: {len(keyword_elements)}ê°œ")
            
            # 7. ë§¥ë„ë‚ ë“œ ê´€ë ¨ ìš”ì†Œ ì°¾ê¸° (í…ŒìŠ¤íŠ¸ìš©)
            print(f"\nğŸ” 'ë§¥ë„ë‚ ë“œ' ê´€ë ¨ ìš”ì†Œ ê²€ìƒ‰:")
            mcdonalds_elements = soup.find_all(text=lambda text: text and 'ë§¥ë„ë‚ ë“œ' in text)
            print(f"  ğŸŸ ë§¥ë„ë‚ ë“œ í¬í•¨ í…ìŠ¤íŠ¸: {len(mcdonalds_elements)}ê°œ")
            
            for i, element in enumerate(mcdonalds_elements[:3]):
                parent = element.parent
                print(f"  {i+1}. í…ìŠ¤íŠ¸: {element.strip()}")
                print(f"     ë¶€ëª¨ íƒœê·¸: {parent.name} (í´ë˜ìŠ¤: {parent.get('class', [])})")
            
            return soup
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def test_selectors(self, soup, keyword):
        """ë‹¤ì–‘í•œ ì„ íƒì í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ§ª ì„ íƒì í…ŒìŠ¤íŠ¸:")
        
        test_selectors = [
            "ul li",
            "div[class*='list'] li", 
            "li[class*='place']",
            "div[class*='place']",
            ".list li",
            ".place_list li",
            "[data-place-id]",
            "a[href*='place']",
            "div:has(a[href*='place'])",
        ]
        
        for selector in test_selectors:
            try:
                elements = soup.select(selector)
                print(f"  ğŸ“‹ {selector:25} : {len(elements):3d}ê°œ")
                
                # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ìš”ì†Œë§Œ ì¹´ìš´íŠ¸
                text_elements = [e for e in elements if e.get_text().strip()]
                if text_elements != elements:
                    print(f"      (í…ìŠ¤íŠ¸ í¬í•¨: {len(text_elements)}ê°œ)")
                    
            except Exception as e:
                print(f"  âŒ {selector:25} : ì˜¤ë¥˜ - {e}")

def main():
    """ë©”ì¸ ë””ë²„ê¹… í•¨ìˆ˜"""
    print("ğŸ”§ ë„¤ì´ë²„ ëª¨ë°”ì¼ í”Œë ˆì´ìŠ¤ í¬ë¡¤ëŸ¬ ë””ë²„ê±°")
    print("=" * 60)
    
    debugger = MobileCrawlerDebugger()
    
    # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_keyword = "ì„œìš¸ ìƒì•” ë§›ì§‘"
    soup = debugger.debug_search(test_keyword)
    
    if soup:
        debugger.test_selectors(soup, test_keyword)
    
    print("\n" + "=" * 60)
    print("ğŸ¯ ë¶„ì„ ì™„ë£Œ!")
    print("1. ìƒì„±ëœ HTML íŒŒì¼ì„ ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸í•˜ì„¸ìš”")
    print("2. ì‹¤ì œ ë„¤ì´ë²„ ëª¨ë°”ì¼ ê²€ìƒ‰ ê²°ê³¼ì™€ ë¹„êµí•˜ì„¸ìš”")
    print("3. ì ì ˆí•œ ì„ íƒìë¥¼ ì°¾ì•„ í¬ë¡¤ëŸ¬ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”")

if __name__ == "__main__":
    main()