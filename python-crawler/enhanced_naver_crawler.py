import requests
import urllib.parse
from bs4 import BeautifulSoup
import json
import time
import random
import os
import logging
from supabase import create_client, Client
from bright_data_proxy_manager import create_bright_data_proxy_manager, BrightDataProxyManager
from proxy_monitor import get_proxy_monitor, log_proxy_request
from bright_data_api_config import setup_bright_data_from_api

class EnhancedNaverPlaceCrawler:
    """Bright Data í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•˜ëŠ” í–¥ìƒëœ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, use_proxy=True):
        self.logger = logging.getLogger("EnhancedNaverPlaceCrawler")
        logging.basicConfig(level=logging.INFO)
        
        # í”„ë¡ì‹œ ì„¤ì •
        self.use_proxy = use_proxy and (os.getenv('BRIGHT_DATA_ENDPOINT') or os.getenv('BRIGHT_DATA_API_KEY'))
        self.proxy_manager = None
        
        if self.use_proxy:
            try:
                # API í‚¤ê°€ ìˆìœ¼ë©´ APIë¥¼ í†µí•´ í”„ë¡ì‹œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
                if os.getenv('BRIGHT_DATA_API_KEY'):
                    self.logger.info("Fetching proxy configurations from Bright Data API...")
                    proxy_configs = setup_bright_data_from_api()
                    if proxy_configs:
                        self.proxy_manager = BrightDataProxyManager(proxy_configs)
                        self.logger.info(f"Proxy manager initialized with {len(proxy_configs)} proxies from API")
                    else:
                        # API ì‹¤íŒ¨ ì‹œ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
                        self.logger.warning("Failed to get proxies from API, falling back to environment variables")
                        self.proxy_manager = create_bright_data_proxy_manager()
                else:
                    # API í‚¤ê°€ ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
                    self.proxy_manager = create_bright_data_proxy_manager()
                    self.logger.info("Proxy manager initialized from environment variables")
            except Exception as e:
                self.logger.warning(f"Failed to initialize proxy manager: {e}")
                self.use_proxy = False
        
        # ê¸°ë³¸ í—¤ë” (í”„ë¡ì‹œê°€ ì—†ì„ ë•Œ ì‚¬ìš©)
        self.default_headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Referer": "https://m.map.naver.com/"
        }
        
        # Supabase ì„¤ì •
        url = os.getenv('SUPABASE_URL')
        key = os.getenv('SUPABASE_SERVICE_KEY')
        if url and key:
            self.supabase: Client = create_client(url, key)
        else:
            self.supabase = None
            self.logger.warning("Supabase credentials not found")
        
        # í”„ë¡ì‹œ ëª¨ë‹ˆí„° ì´ˆê¸°í™”
        self.proxy_monitor = get_proxy_monitor()

    def build_url(self, keyword):
        """ê²€ìƒ‰ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„¤ì´ë²„ ëª¨ë°”ì¼ ì§€ë„ ê²€ìƒ‰ URLì„ ìƒì„±"""
        encoded_keyword = urllib.parse.quote(keyword)
        # ì—¬ëŸ¬ URL íŒ¨í„´ ì‹œë„
        urls = [
            f"https://m.place.naver.com/restaurant/list?query={encoded_keyword}&entry=plt",
            f"https://pcmap.place.naver.com/place/list?query={encoded_keyword}",
            f"https://m.map.naver.com/search2/search.naver?query={encoded_keyword}&sm=hty&style=v5"
        ]
        return urls

    def make_request_with_fallback(self, urls, **kwargs):
        """í”„ë¡ì‹œì™€ ì¼ë°˜ ìš”ì²­ì„ ëª¨ë‘ ì‹œë„í•˜ëŠ” ìš”ì²­ ë©”ì„œë“œ"""
        
        # 1. í”„ë¡ì‹œ ì‚¬ìš© ì‹œë„
        if self.use_proxy and self.proxy_manager:
            for url in urls:
                start_time = time.time()
                try:
                    self.logger.info(f"Trying proxy request to: {url}")
                    response, proxy = self.proxy_manager.make_request(url, **kwargs)
                    response_time = time.time() - start_time
                    
                    if response and response.status_code == 200:
                        self.logger.info(f"Proxy request successful: {url}")
                        # ì„±ê³µ ë¡œê¹…
                        log_proxy_request(
                            proxy_endpoint=proxy.endpoint if proxy else "unknown",
                            request_url=url,
                            status_code=response.status_code,
                            response_time=response_time,
                            success=True,
                            session_id=proxy.session_id if proxy else None,
                            country=proxy.country if proxy else "KR"
                        )
                        return response, 'proxy'
                    else:
                        # ì‹¤íŒ¨ ë¡œê¹…
                        log_proxy_request(
                            proxy_endpoint=proxy.endpoint if proxy else "unknown",
                            request_url=url,
                            status_code=response.status_code if response else None,
                            response_time=response_time,
                            success=False,
                            error_message=f"HTTP {response.status_code}" if response else "No response",
                            session_id=proxy.session_id if proxy else None,
                            country=proxy.country if proxy else "KR"
                        )
                        
                except Exception as e:
                    response_time = time.time() - start_time
                    self.logger.warning(f"Proxy request failed for {url}: {e}")
                    # ì—ëŸ¬ ë¡œê¹…
                    log_proxy_request(
                        proxy_endpoint="unknown",
                        request_url=url,
                        status_code=None,
                        response_time=response_time,
                        success=False,
                        error_message=str(e)
                    )
                    continue
            
            # í”„ë¡ì‹œ ì‹¤íŒ¨ ì‹œ ë¦¬ì…‹ ì‹œë„
            self.logger.info("Resetting failed proxies and retrying...")
            self.proxy_manager.reset_failed_proxies()
        
        # 2. ì¼ë°˜ ìš”ì²­ ì‹œë„ (fallback)
        self.logger.info("Falling back to direct requests")
        session = requests.Session()
        session.headers.update(self.default_headers)
        
        for url in urls:
            try:
                self.logger.info(f"Trying direct request to: {url}")
                response = session.get(url, timeout=15, **kwargs)
                
                if response.status_code == 200:
                    self.logger.info(f"Direct request successful: {url}")
                    return response, 'direct'
                    
            except Exception as e:
                self.logger.warning(f"Direct request failed for {url}: {e}")
                continue
        
        return None, None

    def search_place_rank(self, keyword, shop_name):
        """
        í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ì—¬ íŠ¹ì • ìƒí˜¸ëª…ì˜ ìˆœìœ„ë¥¼ ì°¾ìŒ
        """
        result = {
            "keyword": keyword,
            "shop_name": shop_name,
            "rank": -1,
            "success": False,
            "message": "",
            "search_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "found_shops": [],
            "request_method": "unknown"
        }
        
        try:
            # ê²€ìƒ‰ URL ëª©ë¡ ìƒì„±
            urls = self.build_url(keyword)
            self.logger.info(f"Searching for '{shop_name}' with keyword: '{keyword}'")
            
            # ìš”ì²­ ì‹¤í–‰ (í”„ë¡ì‹œ + fallback)
            response, method = self.make_request_with_fallback(urls)
            result["request_method"] = method
            
            if not response:
                result["message"] = "ëª¨ë“  ìš”ì²­ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                self.logger.error(result["message"])
                return result
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ë””ë²„ê¹…ì„ ìœ„í•œ HTML ì €ì¥ (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
            if os.getenv('DEBUG_MODE') == 'true':
                with open(f"debug_response_{int(time.time())}.html", "w", encoding="utf-8") as f:
                    f.write(response.text)
            
            # ì¥ì†Œ ëª©ë¡ ì°¾ê¸° - ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
            place_items = self._extract_place_items(soup)
            
            if not place_items:
                result["message"] = "ì¥ì†Œ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                self.logger.warning(result["message"])
                return result
            
            # ì¥ì†Œ ìˆœìœ„ ì°¾ê¸°
            rank, found_shops = self._find_place_rank(place_items, shop_name)
            
            result["found_shops"] = found_shops[:20]
            
            if rank > 0:
                result["rank"] = rank
                result["success"] = True
                result["message"] = f"'{shop_name}'ì€(ëŠ”) '{keyword}' ê²€ìƒ‰ ê²°ê³¼ì—ì„œ {rank}ìœ„ì…ë‹ˆë‹¤."
                self.logger.info(result["message"])
            else:
                result["message"] = f"'{shop_name}'ì„(ë¥¼) ìƒìœ„ {len(place_items)}ê°œ ê²°ê³¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                self.logger.info(result["message"])
            
            # í”„ë¡ì‹œ í†µê³„ ë¡œê¹…
            if self.use_proxy and self.proxy_manager:
                stats = self.proxy_manager.get_proxy_stats()
                self.logger.info(f"Proxy stats: {stats['active']}/{stats['total']} active")
                
        except Exception as e:
            result["message"] = f"ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__} - {e}"
            self.logger.error(result["message"])
            import traceback
            traceback.print_exc()
        
        return result

    def _extract_place_items(self, soup):
        """ë‹¤ì–‘í•œ ì„ íƒìë¡œ ì¥ì†Œ ëª©ë¡ ì¶”ì¶œ"""
        place_items = []
        
        # ëª¨ë°”ì¼ê³¼ ë°ìŠ¤í¬í†± ì„ íƒìë“¤
        selectors = [
            # ëª¨ë°”ì¼ ì„ íƒì
            "li[data-index]",
            "li.place_item", 
            "div.place_list li",
            "ul.list_place li",
            ".search_result li",
            ".place_result li",
            
            # ë°ìŠ¤í¬í†± ì„ íƒì
            "li.UEzoS",
            "li.VLTHu", 
            "div.Ryr1F#_pcmap_list_scroll_container > ul > li",
            "ul._3l82D > li",
            "ul._1s-8x > li",
            ".search_list li",
            
            # ì¼ë°˜ì ì¸ ì„ íƒì
            ".result_list li",
            ".place_list_result li",
            "[data-place-id]"
        ]
        
        for selector in selectors:
            items = soup.select(selector)
            if items:
                self.logger.info(f"ì„ íƒì '{selector}'ë¡œ {len(items)}ê°œ í•­ëª© ë°œê²¬")
                place_items = items
                break
        
        # ì„ íƒìë¡œ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ëª¨ë“  li íƒœê·¸ ì‹œë„
        if not place_items:
            all_lis = soup.find_all("li")
            place_items = [li for li in all_lis if li.get_text().strip() and len(li.get_text()) > 10]
            self.logger.info(f"ëª¨ë“  li íƒœê·¸ì—ì„œ {len(place_items)}ê°œ í•­ëª© ë°œê²¬")
        
        return place_items

    def _find_place_rank(self, place_items, shop_name):
        """ì¥ì†Œ ëª©ë¡ì—ì„œ ìƒí˜¸ëª…ì˜ ìˆœìœ„ ì°¾ê¸°"""
        rank = 0
        found_shops = []
        
        for item in place_items[:500]:  # ìƒìœ„ 500ê°œê¹Œì§€ í™•ì¸
            text = item.get_text()
            
            # ê´‘ê³  ì œì™¸
            if any(ad_word in text for ad_word in ["ê´‘ê³ ", "AD", "Sponsored", "ìŠ¤í°ì„œ"]):
                continue
            
            rank += 1
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            clean_text = text.replace("\n", " ").strip()
            found_shops.append(clean_text[:50])  # ì²˜ìŒ 50ìë§Œ ì €ì¥
            
            # ë§¤ì¹­ ì‹œë„
            if self._is_place_match(clean_text, shop_name):
                return rank, found_shops
        
        return -1, found_shops

    def _is_place_match(self, text, shop_name):
        """í…ìŠ¤íŠ¸ì™€ ìƒí˜¸ëª…ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸"""
        # 1. ì§ì ‘ í¬í•¨ ì—¬ë¶€ í™•ì¸
        if shop_name.lower() in text.lower():
            return True
        
        # 2. ê³µë°± ì œê±° í›„ í™•ì¸
        text_no_space = text.replace(" ", "").replace("\t", "")
        shop_no_space = shop_name.replace(" ", "").replace("\t", "")
        
        if shop_no_space.lower() in text_no_space.lower():
            return True
        
        # 3. íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ í™•ì¸
        import re
        text_clean = re.sub(r'[^\wê°€-í£]', '', text)
        shop_clean = re.sub(r'[^\wê°€-í£]', '', shop_name)
        
        if shop_clean.lower() in text_clean.lower():
            return True
        
        return False

    def save_to_supabase(self, results, tracked_place_id=None):
        """ê²°ê³¼ë¥¼ Supabaseì— ì €ì¥"""
        if not self.supabase or not results:
            return False
            
        try:
            for result in results:
                # crawler_results í…Œì´ë¸”ì— ì €ì¥
                insert_data = {
                    'tracked_place_id': tracked_place_id,
                    'keyword': result['keyword'],
                    'place_name': result['shop_name'],
                    'rank': result['rank'] if result['success'] else None,
                    'review_count': 0,
                    'visitor_review_count': 0,
                    'blog_review_count': 0,
                    'crawled_at': result['search_time'],
                    'success': result['success'],
                    'error_message': result['message'] if not result['success'] else None,
                    'request_method': result.get('request_method', 'unknown')
                }
                
                response = self.supabase.table('crawler_results').insert(insert_data).execute()
                
                # rankings í…Œì´ë¸”ì—ë„ ì €ì¥ (ì„±ê³µí•œ ê²½ìš°ë§Œ)
                if tracked_place_id and result['success']:
                    ranking_data = {
                        'place_id': tracked_place_id,  # place_id ì»¬ëŸ¼ ì‚¬ìš©
                        'rank': result['rank'],
                        'checked_at': result['search_time']
                    }
                    self.supabase.table('rankings').insert(ranking_data).execute()
                    
            self.logger.info(f"Saved {len(results)} results to Supabase")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to save to Supabase: {str(e)}")
            return False

    def crawl_tracked_places(self):
        """ë“±ë¡ëœ tracked_placesë¥¼ ëª¨ë‘ í¬ë¡¤ë§"""
        if not self.supabase:
            self.logger.error("Supabase not configured")
            return
            
        try:
            # í™œì„±í™”ëœ tracked_places ê°€ì ¸ì˜¤ê¸°
            response = self.supabase.table('tracked_places').select('*').eq('is_active', True).execute()
            tracked_places = response.data
            
            self.logger.info(f"Found {len(tracked_places)} active tracked places")
            
            if not tracked_places:
                self.logger.warning("No active tracked places found")
                return
            
            success_count = 0
            total_count = len(tracked_places)
            
            for i, place in enumerate(tracked_places, 1):
                keyword = place['search_keyword']
                place_name = place['place_name']
                place_id = place['id']
                
                self.logger.info(f"\n[{i}/{total_count}] í¬ë¡¤ë§ ì‹œì‘: {place_name} (í‚¤ì›Œë“œ: {keyword})")
                
                # ê²€ìƒ‰ ì‹¤í–‰
                result = self.search_place_rank(keyword, place_name)
                
                # ê²°ê³¼ ì €ì¥
                if self.save_to_supabase([result], place_id):
                    if result['success']:
                        success_count += 1
                        self.logger.info(f"âœ… ì„±ê³µ: {place_name} - {result['rank']}ìœ„")
                    else:
                        self.logger.warning(f"âŒ ì‹¤íŒ¨: {place_name} - {result['message']}")
                
                # ìš”ì²­ ê°„ê²© ì¡°ì • (í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼)
                if self.use_proxy:
                    delay = random.uniform(1, 3)  # í”„ë¡ì‹œ ì‚¬ìš© ì‹œ ì§§ì€ ëŒ€ê¸°
                else:
                    delay = random.uniform(5, 10)  # ì§ì ‘ ìš”ì²­ ì‹œ ê¸´ ëŒ€ê¸°
                
                if i < total_count:  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ëŒ€ê¸°
                    self.logger.info(f"ë‹¤ìŒ ìš”ì²­ê¹Œì§€ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(delay)
            
            # ìµœì¢… ê²°ê³¼ ë¦¬í¬íŠ¸
            self.logger.info(f"\nğŸ¯ í¬ë¡¤ë§ ì™„ë£Œ: {success_count}/{total_count} ì„±ê³µ")
            
            # í”„ë¡ì‹œ í†µê³„ ì¶œë ¥ ë° ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ìƒì„±
            if self.use_proxy and self.proxy_manager:
                stats = self.proxy_manager.get_proxy_stats()
                self.logger.info(f"ğŸ“Š í”„ë¡ì‹œ í†µê³„: {stats}")
                
                # ì„¸ì…˜ë³„ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ìƒì„±
                monitor_stats = self.proxy_monitor.get_usage_stats(hours=1)  # ìµœê·¼ 1ì‹œê°„
                monitor_report = self.proxy_monitor.export_usage_report(hours=1, format='text')
                self.logger.info(f"\n{monitor_report}")
                
                # ì¼ì¼ ìš”ì•½ ì €ì¥ (ì„ íƒì )
                if os.getenv('SAVE_DAILY_SUMMARY', 'false').lower() == 'true':
                    self.proxy_monitor.save_daily_summary()
                
        except Exception as e:
            self.logger.error(f"Crawl tracked places failed: {str(e)}")
            import traceback
            traceback.print_exc()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
    use_proxy = os.getenv('USE_BRIGHT_DATA_PROXY', 'true').lower() == 'true'
    
    crawler = EnhancedNaverPlaceCrawler(use_proxy=use_proxy)
    
    # í™˜ê²½ë³€ìˆ˜ë¡œ ëª¨ë“œ ê²°ì •
    mode = os.getenv('CRAWLER_MODE', 'tracked')
    
    if mode == 'test':
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        keyword = os.getenv('TEST_KEYWORD', 'ì„œìš¸ ìƒì•” ë§›ì§‘')
        shop_name = os.getenv('TEST_SHOP_NAME', 'ë§¥ë„ë‚ ë“œìƒì•”DMCì ')
        
        print(f"ğŸ” í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹œì‘: {shop_name} (í‚¤ì›Œë“œ: {keyword})")
        result = crawler.search_place_rank(keyword, shop_name)
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
    else:
        # ì‹¤ì œ í¬ë¡¤ë§ ëª¨ë“œ
        print("ğŸš€ ì‹¤ì œ í¬ë¡¤ë§ ì‹œì‘")
        crawler.crawl_tracked_places()

if __name__ == "__main__":
    main()