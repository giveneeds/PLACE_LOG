#!/usr/bin/env python3
"""
2025ë…„ 5ì›” ë„¤ì´ë²„ ì—…ë°ì´íŠ¸ ë°˜ì˜ + IP ë¡œí…Œì´ì…˜ ì§€ì› í¬ë¡¤ëŸ¬
- ìµœì‹  ì…€ë ‰í„°: li[data-nclick*="plc"]
- IP ë¡œí…Œì´ì…˜ ë° CAPTCHA íšŒí”¼
- 500 ìš”ì²­/ì¼ ì œí•œ ê³ ë ¤í•œ ìš”ì²­ ê´€ë¦¬
"""
import time
import re
import random
import os
import logging
import json
import hashlib
from typing import Dict, List, Optional, Union
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.proxy import Proxy, ProxyType
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from supabase import create_client, Client

class Updated2025NaverCrawler:
    """
    2025ë…„ 5ì›” ë„¤ì´ë²„ ì—…ë°ì´íŠ¸ ë°˜ì˜ í¬ë¡¤ëŸ¬
    - ìµœì‹  HTML êµ¬ì¡° ì§€ì›
    - IP ë¡œí…Œì´ì…˜ ì‹œìŠ¤í…œ
    - CAPTCHA íšŒí”¼ ì „ëµ
    """
    
    def __init__(self, headless=True, delay_range=(5, 15), use_proxy=False, proxy_list=None):
        self.delay_range = delay_range
        self.use_proxy = use_proxy
        self.proxy_list = proxy_list or []
        self.current_proxy_index = 0
        self.request_count = 0
        self.daily_request_limit = 450  # ì•ˆì „ ë§ˆì§„ì„ ë‘ê³  450ê°œë¡œ ì œí•œ
        self.last_reset_date = datetime.now().date()
        
        self.logger = self._setup_logging()
        self.driver = None
        self.setup_driver(headless)
        
        # Supabase ì„¤ì •
        url = os.getenv('SUPABASE_URL')
        key = os.getenv('SUPABASE_SERVICE_KEY') 
        if url and key:
            self.supabase: Client = create_client(url, key)
        else:
            self.supabase = None
            self.logger.warning("Supabase credentials not found")
        
        # ì‚¬ìš©ì ì—ì´ì „íŠ¸ í’€ (2025ë…„ 5ì›” ê¸°ì¤€ ìµœì‹ )
        self.user_agents = [
            "Mozilla/5.0 (iPhone; CPU iPhone OS 17_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Mobile/15E148 Safari/604.1",
            "Mozilla/5.0 (iPhone; CPU iPhone OS 17_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Mobile/15E148 Safari/604.1",
            "Mozilla/5.0 (Android 14; Mobile; rv:109.0) Gecko/109.0 Firefox/115.0",
            "Mozilla/5.0 (Android 13; Mobile; rv:109.0) Gecko/109.0 Firefox/114.0",
            "Mozilla/5.0 (Linux; Android 14; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Mobile Safari/537.36",
            "Mozilla/5.0 (Linux; Android 13; Pixel 7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36"
        ]
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger("Updated2025NaverCrawler")
    
    def _check_daily_limit(self):
        """ì¼ì¼ ìš”ì²­ ì œí•œ í™•ì¸"""
        today = datetime.now().date()
        
        # ë‚ ì§œê°€ ë°”ë€Œë©´ ì¹´ìš´í„° ë¦¬ì…‹
        if today > self.last_reset_date:
            self.request_count = 0
            self.last_reset_date = today
            self.logger.info("Daily request counter reset")
        
        if self.request_count >= self.daily_request_limit:
            self.logger.warning(f"Daily request limit ({self.daily_request_limit}) reached. Switching IP or waiting...")
            
            if self.use_proxy and len(self.proxy_list) > 1:
                self._rotate_proxy()
                self.request_count = 0  # í”„ë¡ì‹œ ë³€ê²½ ì‹œ ì¹´ìš´í„° ë¦¬ì…‹
            else:
                # í”„ë¡ì‹œê°€ ì—†ìœ¼ë©´ ë‹¤ìŒ ë‚ ê¹Œì§€ ëŒ€ê¸°
                tomorrow = datetime.combine(today + timedelta(days=1), datetime.min.time())
                wait_seconds = (tomorrow - datetime.now()).total_seconds()
                self.logger.info(f"Waiting {wait_seconds/3600:.1f} hours until tomorrow...")
                time.sleep(min(wait_seconds, 3600))  # ìµœëŒ€ 1ì‹œê°„ë§Œ ëŒ€ê¸°
                return False
        
        return True
    
    def _rotate_proxy(self):
        """í”„ë¡ì‹œ ë¡œí…Œì´ì…˜"""
        if not self.proxy_list:
            return False
        
        self.current_proxy_index = (self.current_proxy_index + 1) % len(self.proxy_list)
        new_proxy = self.proxy_list[self.current_proxy_index]
        
        self.logger.info(f"Rotating to proxy: {new_proxy}")
        
        # WebDriver ì¬ì‹œì‘
        if self.driver:
            self.driver.quit()
        
        self.setup_driver(headless=True, proxy=new_proxy)
        return True
    
    def setup_driver(self, headless, proxy=None):
        """Chrome WebDriver ì„¤ì • (2025ë…„ 5ì›” ìµœì í™”)"""
        options = Options()
        
        if headless:
            options.add_argument('--headless')
        
        # 2025ë…„ 5ì›” ê¸°ì¤€ ìµœì‹  ë´‡ íƒì§€ ìš°íšŒ ì„¤ì •
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--disable-features=VizDisplayCompositor')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        # ì¶”ê°€ íƒì§€ ìš°íšŒ ì„¤ì •
        options.add_argument('--disable-web-security')
        options.add_argument('--allow-running-insecure-content')
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-plugins')
        options.add_argument('--disable-images')  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
        options.add_argument('--disable-javascript')  # ì¼ë¶€ JS ë¹„í™œì„±í™”
        
        # ëœë¤ User-Agent ì„ íƒ
        user_agent = random.choice(self.user_agents)
        options.add_argument(f'--user-agent={user_agent}')
        
        # ëª¨ë°”ì¼ ë·°í¬íŠ¸ ì„¤ì •
        mobile_sizes = [
            (375, 812),  # iPhone 13 mini
            (390, 844),  # iPhone 14
            (393, 873),  # Pixel 7
            (360, 800),  # Galaxy S21
        ]
        width, height = random.choice(mobile_sizes)
        options.add_argument(f'--window-size={width},{height}')
        
        # í”„ë¡ì‹œ ì„¤ì •
        if proxy and self.use_proxy:
            options.add_argument(f'--proxy-server={proxy}')
        
        # ì¶”ê°€ í—¤ë” ì„¤ì •
        options.add_argument('--accept-lang=ko-KR,ko;q=0.9,en;q=0.8')
        
        try:
            self.driver = webdriver.Chrome(options=options)
            
            # ì¶”ê°€ ë´‡ íƒì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸
            self.driver.execute_script("""
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
                Object.defineProperty(navigator, 'languages', {get: () => ['ko-KR', 'ko', 'en-US', 'en']});
                window.chrome = {runtime: {}};
            """)
            
            self.driver.implicitly_wait(10)
            self.logger.info(f"Chrome WebDriver initialized with User-Agent: {user_agent}")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize WebDriver: {e}")
            raise
    
    def search_place_rank(self, keyword: str, shop_name: str) -> Dict:
        """
        2025ë…„ 5ì›” ë„¤ì´ë²„ ì—…ë°ì´íŠ¸ ë°˜ì˜ ê²€ìƒ‰ í•¨ìˆ˜
        """
        # ì¼ì¼ ìš”ì²­ ì œí•œ í™•ì¸
        if not self._check_daily_limit():
            return {
                "keyword": keyword,
                "shop_name": shop_name,
                "rank": -1,
                "success": False,
                "message": "Daily request limit reached",
                "search_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "found_shops": []
            }
        
        self.request_count += 1
        
        result = {
            "keyword": keyword,
            "shop_name": shop_name,
            "rank": -1,
            "success": False,
            "message": "",
            "search_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "found_shops": [],
            "request_count": self.request_count
        }
        
        try:
            self.logger.info(f"Starting search [{self.request_count}/{self.daily_request_limit}]: '{shop_name}' with keyword '{keyword}'")
            
            # 2025ë…„ 5ì›” ê¸°ì¤€ ë„¤ì´ë²„ ëª¨ë°”ì¼ ê²€ìƒ‰ URL
            search_url = f"https://m.search.naver.com/search.naver?where=m&sm=top_sly.hst&fbm=0&acr=1&ie=utf8&query={keyword}"
            
            self.driver.get(search_url)
            self._enhanced_random_delay()
            
            # CAPTCHA ê°ì§€
            if self._detect_captcha():
                result["message"] = "CAPTCHA detected - need to rotate IP"
                self.logger.warning("CAPTCHA detected!")
                
                if self.use_proxy:
                    self._rotate_proxy()
                
                return result
            
            # í”Œë ˆì´ìŠ¤ ì„¹ì…˜ìœ¼ë¡œ ì´ë™
            if not self._navigate_to_place_list_2025():
                result["message"] = "í”Œë ˆì´ìŠ¤ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                return result
            
            # 2025ë…„ 5ì›” ì—…ë°ì´íŠ¸ëœ ìˆœìœ„ ê²€ìƒ‰
            rank_result = self._find_place_rank_2025(shop_name)
            result.update(rank_result)
            
            if result["success"]:
                self.logger.info(f"Found '{shop_name}' at rank {result['rank']}")
            else:
                self.logger.warning(f"Could not find '{shop_name}'")
                
        except Exception as e:
            result["message"] = f"ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__} - {e}"
            self.logger.error(f"Error in search_place_rank: {e}")
        
        return result
    
    def _detect_captcha(self) -> bool:
        """CAPTCHA ê°ì§€"""
        try:
            # URL ê¸°ë°˜ ê°ì§€
            current_url = self.driver.current_url.lower()
            if any(keyword in current_url for keyword in ['captcha', 'block', 'verify', 'robot']):
                return True
            
            # í˜ì´ì§€ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°ì§€
            page_text = self.driver.page_source.lower()
            captcha_keywords = [
                'captcha', 'ë³´ì•ˆë¬¸ì', 'ìë™ì…ë ¥', 'ë¡œë´‡', 'robot',
                'verify', 'ì¸ì¦', 'block', 'ì°¨ë‹¨', 'access denied'
            ]
            
            for keyword in captcha_keywords:
                if keyword in page_text:
                    return True
            
            # íŠ¹ì • ìš”ì†Œ ê¸°ë°˜ ê°ì§€
            captcha_selectors = [
                "[class*='captcha']",
                "[id*='captcha']", 
                "[class*='verify']",
                "[class*='robot']"
            ]
            
            for selector in captcha_selectors:
                if self.driver.find_elements(By.CSS_SELECTOR, selector):
                    return True
            
            return False
            
        except Exception:
            return False
    
    def _navigate_to_place_list_2025(self) -> bool:
        """2025ë…„ 5ì›” ì—…ë°ì´íŠ¸ëœ í”Œë ˆì´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì´ë™"""
        try:
            # ë°©ë²• 1: í”Œë ˆì´ìŠ¤ ë”ë³´ê¸° ë²„íŠ¼ (2025ë…„ 5ì›” ì—…ë°ì´íŠ¸ëœ ì„ íƒì)
            place_more_selectors = [
                "a[href*='place'][href*='list']",
                ".place_area a[href*='more']",
                "#place-main-section-root a",
                ".place_section a:contains('ë”ë³´ê¸°')",
                "a[data-nclick*='place']"
            ]
            
            for selector in place_more_selectors:
                try:
                    if "contains" in selector:
                        elements = self.driver.find_elements(By.XPATH, "//a[contains(text(), 'ë”ë³´ê¸°') and contains(@href, 'place')]")
                    else:
                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    
                    for element in elements:
                        try:
                            href = element.get_attribute('href') or ''
                            text = element.text.strip().lower()
                            
                            if 'place' in href and ('ë”ë³´ê¸°' in text or 'more' in text or len(text) == 0):
                                self.logger.info(f"Clicking place more button: {href}")
                                self.driver.execute_script("arguments[0].click();", element)
                                self._enhanced_random_delay()
                                return True
                        except:
                            continue
                except:
                    continue
            
            # ë°©ë²• 2: ì§ì ‘ í”Œë ˆì´ìŠ¤ ë¦¬ìŠ¤íŠ¸ URL (2025ë…„ 5ì›” íŒ¨í„´)
            current_url = self.driver.current_url
            if 'm.search.naver.com' in current_url:
                import urllib.parse
                parsed = urllib.parse.urlparse(current_url)
                query_params = urllib.parse.parse_qs(parsed.query)
                query = query_params.get('query', [''])[0]
                
                if query:
                    # 2025ë…„ 5ì›” ê¸°ì¤€ í”Œë ˆì´ìŠ¤ ë¦¬ìŠ¤íŠ¸ URL íŒ¨í„´ë“¤
                    place_urls = [
                        f"https://m.place.naver.com/list?query={query}&entry=pll",
                        f"https://m.place.naver.com/restaurant/list?query={query}",
                        f"https://m.search.naver.com/search.naver?where=place&query={query}&sm=tab_opt"
                    ]
                    
                    for place_url in place_urls:
                        try:
                            self.logger.info(f"Trying direct URL: {place_url}")
                            self.driver.get(place_url)
                            self._enhanced_random_delay()
                            
                            # 2025ë…„ 5ì›” ì—…ë°ì´íŠ¸ëœ ì„ íƒìë¡œ í”Œë ˆì´ìŠ¤ ì•„ì´í…œ í™•ì¸
                            if self._get_place_items_2025():
                                return True
                        except:
                            continue
            
            return False
            
        except Exception as e:
            self.logger.error(f"Error navigating to place list: {e}")
            return False
    
    def _get_place_items_2025(self) -> List:
        """2025ë…„ 5ì›” ì—…ë°ì´íŠ¸ëœ í”Œë ˆì´ìŠ¤ ì•„ì´í…œ ì„ íƒì"""
        # 2025ë…„ 5ì›” ê¸°ì¤€ ìµœì‹  ì„ íƒìë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
        selectors_2025 = [
            'li[data-nclick*="plc"]',  # ğŸ¯ ë©”ì¸ ì„ íƒì (2025ë…„ 5ì›” ì—…ë°ì´íŠ¸)
            'li.place_unit',           # ë°±ì—… ì„ íƒì 1
            'li[data-place-id]',       # ë°±ì—… ì„ íƒì 2
            'ul.list_place li',        # ë°±ì—… ì„ íƒì 3
            '.place_list li',          # ë°±ì—… ì„ íƒì 4
            'li.place_item',           # ë°±ì—… ì„ íƒì 5
        ]
        
        for selector in selectors_2025:
            try:
                items = self.driver.find_elements(By.CSS_SELECTOR, selector)
                
                # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ì‹¤ì œ ì•„ì´í…œë§Œ í•„í„°ë§
                valid_items = []
                for item in items:
                    try:
                        text = item.text.strip()
                        if text and len(text) > 10:  # ìµœì†Œ 10ì ì´ìƒì˜ í…ìŠ¤íŠ¸ê°€ ìˆì–´ì•¼ ìœ íš¨
                            valid_items.append(item)
                    except:
                        continue
                
                if valid_items and len(valid_items) >= 3:  # ìµœì†Œ 3ê°œ ì´ìƒì˜ ìœ íš¨í•œ ì•„ì´í…œ
                    self.logger.info(f"Found {len(valid_items)} items with selector: {selector}")
                    return valid_items
                    
            except Exception as e:
                self.logger.debug(f"Error with selector {selector}: {e}")
                continue
        
        self.logger.warning("No place items found with any 2025 selectors")
        return []
    
    def _find_place_rank_2025(self, target_shop_name: str) -> Dict:
        """2025ë…„ 5ì›” ì—…ë°ì´íŠ¸ ë°˜ì˜ ìˆœìœ„ ê²€ìƒ‰"""
        result = {
            "rank": -1,
            "success": False,
            "message": "",
            "found_shops": []
        }
        
        current_rank = 1
        found_shops = []
        scroll_count = 0
        max_scrolls = 8  # ìŠ¤í¬ë¡¤ íšŸìˆ˜ ì œí•œ
        
        while scroll_count < max_scrolls and current_rank <= 50:  # ìƒìœ„ 50ê°œê¹Œì§€ë§Œ
            # 2025ë…„ 5ì›” ì—…ë°ì´íŠ¸ëœ í”Œë ˆì´ìŠ¤ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°
            place_items = self._get_place_items_2025()
            
            if not place_items:
                self.logger.warning("No place items found")
                break
            
            # ìƒˆë¡œìš´ ì•„ì´í…œë“¤ë§Œ ì²˜ë¦¬
            for item in place_items[len(found_shops):]:
                try:
                    place_info = self._extract_place_info_2025(item)
                    if not place_info:
                        continue
                    
                    place_name = place_info['name']
                    is_ad = place_info.get('is_ad', False)
                    
                    # ê´‘ê³ ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ ìˆœìœ„ì— í¬í•¨
                    if not is_ad:
                        found_shops.append(place_name)
                        
                        # ìœ ì—°í•œ ë§¤ì¹­ (2025ë…„ ì—…ë°ì´íŠ¸)
                        if self._is_match_2025(target_shop_name, place_name):
                            result.update({
                                "rank": current_rank,
                                "success": True,
                                "message": f"'{target_shop_name}'ì€(ëŠ”) '{self._get_search_keyword()}' ê²€ìƒ‰ ê²°ê³¼ì—ì„œ {current_rank}ìœ„ì…ë‹ˆë‹¤.",
                                "found_shops": found_shops[:10]
                            })
                            return result
                        
                        current_rank += 1
                        
                        # 50ìœ„ê¹Œì§€ë§Œ í™•ì¸
                        if current_rank > 50:
                            break
                
                except Exception as e:
                    self.logger.debug(f"Error processing place item: {e}")
                    continue
            
            # ë” ë§ì€ ê²°ê³¼ë¥¼ ìœ„í•œ ìŠ¤í¬ë¡¤
            if not self._scroll_with_loading_wait():
                break
            
            scroll_count += 1
            self._enhanced_random_delay(min_delay=2, max_delay=5)
        
        # ì°¾ì§€ ëª»í•œ ê²½ìš°
        result.update({
            "found_shops": found_shops[:20],
            "message": f"'{target_shop_name}'ì„(ë¥¼) ìƒìœ„ {min(current_rank-1, 50)}ê°œ ê²°ê³¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        })
        
        return result
    
    def _extract_place_info_2025(self, item) -> Optional[Dict]:
        """2025ë…„ 5ì›” ì—…ë°ì´íŠ¸ëœ í”Œë ˆì´ìŠ¤ ì •ë³´ ì¶”ì¶œ"""
        try:
            # 2025ë…„ 5ì›” ê¸°ì¤€ í”Œë ˆì´ìŠ¤ëª… ì„ íƒìë“¤
            name_selectors_2025 = [
                ".place_bluelink",         # ë©”ì¸ ì„ íƒì
                ".place_name",             # ë°±ì—… 1
                ".name",                   # ë°±ì—… 2
                "a[href*='place'] span",   # ë°±ì—… 3
                "strong",                  # ë°±ì—… 4
                "h3",                      # ë°±ì—… 5
                ".title"                   # ë°±ì—… 6
            ]
            
            place_name = None
            for selector in name_selectors_2025:
                try:
                    element = item.find_element(By.CSS_SELECTOR, selector)
                    text = element.text.strip()
                    if text and len(text) > 1 and not text.isdigit():
                        place_name = text
                        break
                except:
                    continue
            
            # ì„ íƒìë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
            if not place_name:
                full_text = item.text.strip()
                if full_text:
                    lines = [line.strip() for line in full_text.split('\n') if line.strip()]
                    if lines:
                        # ì²« ë²ˆì§¸ ì¤„ì—ì„œ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        place_name = max(lines[0].split(), key=len) if lines[0] else None
            
            if not place_name or len(place_name) < 2:
                return None
            
            # 2025ë…„ 5ì›” ì—…ë°ì´íŠ¸ëœ ê´‘ê³  ê°ì§€
            is_ad = self._is_advertisement_2025(item)
            
            return {
                'name': place_name,
                'is_ad': is_ad
            }
            
        except Exception as e:
            self.logger.debug(f"Error extracting place info: {e}")
            return None
    
    def _is_advertisement_2025(self, item) -> bool:
        """2025ë…„ 5ì›” ì—…ë°ì´íŠ¸ëœ ê´‘ê³  ê°ì§€"""
        try:
            # 2025ë…„ 5ì›” ê¸°ì¤€ ê´‘ê³  íŒ¨í„´ë“¤
            ad_patterns_2025 = [
                "[class*='ad']",
                "[class*='sponsor']",
                "[class*='promotion']",
                ".ad_marker",
                ".ad_badge",
                "[data-ad]",
                "[data-sponsor]"
            ]
            
            # CSS ì„ íƒì ê¸°ë°˜ ê°ì§€
            for pattern in ad_patterns_2025:
                if item.find_elements(By.CSS_SELECTOR, pattern):
                    return True
            
            # í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°ì§€ (2025ë…„ íŒ¨í„´)
            item_text = item.text.lower()
            ad_keywords_2025 = [
                'ê´‘ê³ ', 'ad', 'sponsored', 'promotion', 
                'í™ë³´', 'í˜‘ì°¬', 'pr', 'ìŠ¤í°ì„œ'
            ]
            
            for keyword in ad_keywords_2025:
                if keyword in item_text:
                    return True
            
            # data-nclick ì†ì„±ì—ì„œ ê´‘ê³  ê°ì§€
            nclick_attr = item.get_attribute('data-nclick')
            if nclick_attr and any(ad_word in nclick_attr.lower() for ad_word in ['ad', 'sponsor', 'promotion']):
                return True
            
            return False
            
        except Exception:
            return False
    
    def _is_match_2025(self, target_name: str, found_name: str) -> bool:
        """2025ë…„ ì—…ë°ì´íŠ¸ëœ ìœ ì—°í•œ ë§¤ì¹­ ë¡œì§"""
        if not target_name or not found_name:
            return False
        
        # ì •ê·œí™” í•¨ìˆ˜ (í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ)
        def normalize_2025(text):
            # íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ì†Œë¬¸ì ë³€í™˜
            normalized = re.sub(r'[^\wê°€-í£0-9]', '', text.lower())
            return normalized
        
        target_norm = normalize_2025(target_name)
        found_norm = normalize_2025(found_name)
        
        # 1. ì •í™•í•œ ë§¤ì¹˜
        if target_norm == found_norm:
            return True
        
        # 2. í¬í•¨ ê´€ê³„ (ìµœì†Œ 4ê¸€ì)
        if len(target_norm) >= 4 and len(found_norm) >= 4:
            if target_norm in found_norm or found_norm in target_norm:
                return True
        
        # 3. Jaccard ìœ ì‚¬ë„ (2ê¸€ì ì¡°í•©)
        if len(target_norm) >= 4 and len(found_norm) >= 4:
            target_bigrams = set(target_norm[i:i+2] for i in range(len(target_norm)-1))
            found_bigrams = set(found_norm[i:i+2] for i in range(len(found_norm)-1))
            
            if target_bigrams and found_bigrams:
                intersection = target_bigrams.intersection(found_bigrams)
                union = target_bigrams.union(found_bigrams)
                similarity = len(intersection) / len(union)
                
                if similarity >= 0.7:  # 70% ì´ìƒ ìœ ì‚¬
                    return True
        
        # 4. í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹˜ (ë¸Œëœë“œëª… ë“±)
        target_keywords = re.findall(r'[ê°€-í£]{2,}|[a-zA-Z]{3,}', target_name)
        found_keywords = re.findall(r'[ê°€-í£]{2,}|[a-zA-Z]{3,}', found_name)
        
        for target_kw in target_keywords:
            for found_kw in found_keywords:
                if target_kw.lower() in found_kw.lower() or found_kw.lower() in target_kw.lower():
                    if len(target_kw) >= 3:  # ìµœì†Œ 3ê¸€ì í‚¤ì›Œë“œ
                        return True
        
        return False
    
    def _scroll_with_loading_wait(self) -> bool:
        """ë¡œë”© ëŒ€ê¸°ê°€ í¬í•¨ëœ ìŠ¤í¬ë¡¤"""
        try:
            # í˜„ì¬ ì•„ì´í…œ ê°œìˆ˜ ì €ì¥
            initial_items = len(self._get_place_items_2025())
            
            # ìŠ¤í¬ë¡¤ ì‹¤í–‰
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # ë¡œë”© ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ)
            for _ in range(20):  # 0.5ì´ˆì”© 20ë²ˆ = 10ì´ˆ
                time.sleep(0.5)
                current_items = len(self._get_place_items_2025())
                
                if current_items > initial_items:
                    self.logger.debug(f"New items loaded: {initial_items} -> {current_items}")
                    return True
            
            return False  # ìƒˆ ì•„ì´í…œì´ ë¡œë“œë˜ì§€ ì•ŠìŒ
            
        except Exception as e:
            self.logger.debug(f"Scroll failed: {e}")
            return False
    
    def _enhanced_random_delay(self, min_delay=None, max_delay=None):
        """í–¥ìƒëœ ëœë¤ ì§€ì—° (CAPTCHA íšŒí”¼)"""
        if min_delay is None or max_delay is None:
            min_delay, max_delay = self.delay_range
        
        # ê¸°ë³¸ ì§€ì—°
        delay = random.uniform(min_delay, max_delay)
        
        # ìš”ì²­ íšŸìˆ˜ì— ë”°ë¥¸ ì¶”ê°€ ì§€ì—° (CAPTCHA íšŒí”¼)
        if self.request_count > 100:
            delay += random.uniform(5, 10)  # 100íšŒ ì´í›„ 5-10ì´ˆ ì¶”ê°€
        elif self.request_count > 200:
            delay += random.uniform(10, 20)  # 200íšŒ ì´í›„ 10-20ì´ˆ ì¶”ê°€
        elif self.request_count > 300:
            delay += random.uniform(20, 40)  # 300íšŒ ì´í›„ 20-40ì´ˆ ì¶”ê°€
        
        self.logger.debug(f"Waiting {delay:.2f} seconds...")
        time.sleep(delay)
    
    def _get_search_keyword(self) -> str:
        """í˜„ì¬ URLì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            current_url = self.driver.current_url
            import urllib.parse
            parsed = urllib.parse.urlparse(current_url)
            query_params = urllib.parse.parse_qs(parsed.query)
            return query_params.get('query', [''])[0]
        except:
            return ''
    
    def save_to_supabase(self, results: Union[List[Dict], Dict], tracked_place_id: Optional[int] = None) -> bool:
        """ê²°ê³¼ë¥¼ Supabaseì— ì €ì¥ (ìš”ì²­ íšŸìˆ˜ ì •ë³´ í¬í•¨)"""
        if not self.supabase:
            return False
        
        if isinstance(results, dict):
            results = [results]
            
        if not results:
            return False
            
        try:
            for result in results:
                # crawler_results í…Œì´ë¸”ì— ì €ì¥
                insert_data = {
                    'tracked_place_id': tracked_place_id,
                    'keyword': result['keyword'],
                    'place_name': result['shop_name'],
                    'rank': result['rank'] if result['success'] else None,
                    'review_count': 0,
                    'visitor_review_count': 0,
                    'blog_review_count': 0,
                    'crawled_at': result['search_time'],
                    'success': result['success'],
                    'error_message': result['message'] if not result['success'] else None,
                    'request_count': result.get('request_count', self.request_count)  # ìš”ì²­ íšŸìˆ˜ ì¶”ê°€
                }
                
                self.supabase.table('crawler_results').insert(insert_data).execute()
                
                # rankings í…Œì´ë¸”ì—ë„ ì €ì¥ (ì„±ê³µí•œ ê²½ìš°ë§Œ)
                if tracked_place_id and result['success']:
                    ranking_data = {
                        'tracked_place_id': tracked_place_id,
                        'rank': result['rank'],
                        'checked_at': result['search_time']
                    }
                    self.supabase.table('rankings').insert(ranking_data).execute()
                    
            self.logger.info(f"Saved {len(results)} results to Supabase")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to save to Supabase: {e}")
            return False
    
    def crawl_tracked_places(self):
        """ë“±ë¡ëœ tracked_placesë¥¼ ëª¨ë‘ í¬ë¡¤ë§ (ê°œì„ ëœ ë²„ì „)"""
        if not self.supabase:
            self.logger.error("Supabase not configured")
            return
            
        try:
            # í™œì„±í™”ëœ tracked_places ê°€ì ¸ì˜¤ê¸°
            response = self.supabase.table('tracked_places').select('*').eq('is_active', True).execute()
            tracked_places = response.data
            
            self.logger.info(f"Found {len(tracked_places)} active tracked places")
            
            if len(tracked_places) > self.daily_request_limit - self.request_count:
                self.logger.warning(f"Too many places to crawl today. Limiting to {self.daily_request_limit - self.request_count}")
                tracked_places = tracked_places[:self.daily_request_limit - self.request_count]
            
            for i, place in enumerate(tracked_places, 1):
                keyword = place['search_keyword']
                place_name = place['place_name']
                place_id = place['id']
                
                self.logger.info(f"í¬ë¡¤ë§ [{i}/{len(tracked_places)}]: {place_name} (í‚¤ì›Œë“œ: {keyword})")
                
                # ê²€ìƒ‰ ì‹¤í–‰
                result = self.search_place_rank(keyword, place_name)
                
                # ê²°ê³¼ ì €ì¥
                self.save_to_supabase(result, place_id)
                
                # CAPTCHA ë°œìƒ ì‹œ ì¤‘ë‹¨
                if "CAPTCHA detected" in result.get('message', ''):
                    self.logger.error("CAPTCHA detected. Stopping crawling session.")
                    break
                
                # ì¼ì¼ í•œë„ ë„ë‹¬ ì‹œ ì¤‘ë‹¨  
                if self.request_count >= self.daily_request_limit:
                    self.logger.info("Daily request limit reached. Stopping.")
                    break
                
                # í–¥ìƒëœ ì§€ì—°
                self._enhanced_random_delay()
                
        except Exception as e:
            self.logger.error(f"Crawl tracked places failed: {e}")
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.driver:
                self.driver.quit()
                self.logger.info(f"WebDriver closed. Total requests made: {self.request_count}")
        except Exception as e:
            self.logger.error(f"Error closing WebDriver: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (2025ë…„ 5ì›” ì—…ë°ì´íŠ¸)"""
    
    # í”„ë¡ì‹œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆì‹œ - ì‹¤ì œ í”„ë¡ì‹œë¡œ êµì²´ í•„ìš”)
    proxy_list = [
        # "http://proxy1:port",
        # "http://proxy2:port", 
        # "http://proxy3:port"
    ]
    
    # í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€
    use_proxy = len(proxy_list) > 0
    
    crawler = Updated2025NaverCrawler(
        headless=True, 
        delay_range=(8, 20),  # ë” ê¸´ ì§€ì—° ì‹œê°„
        use_proxy=use_proxy,
        proxy_list=proxy_list
    )
    
    try:
        mode = os.getenv('CRAWLER_MODE', 'test')
        
        if mode == 'test':
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            keyword = os.getenv('TEST_KEYWORD', 'ì„œìš¸ ìƒì•” ë§›ì§‘')
            shop_name = os.getenv('TEST_SHOP_NAME', 'ë§¥ë„ë‚ ë“œìƒì•”DMCì ')
            
            result = crawler.search_place_rank(keyword, shop_name)
            
            print("=== 2025ë…„ 5ì›” ì—…ë°ì´íŠ¸ í¬ë¡¤ë§ ê²°ê³¼ ===")
            print(json.dumps(result, ensure_ascii=False, indent=2))
            
        else:
            # ì‹¤ì œ í¬ë¡¤ë§ ëª¨ë“œ
            crawler.crawl_tracked_places()
            
    finally:
        crawler.close()

if __name__ == "__main__":
    main()