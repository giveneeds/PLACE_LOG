#!/usr/bin/env python3
"""
í¬ë¡¤ëŸ¬ ë¬¸ì œ ë¶„ì„ ë„êµ¬
ì‚¬ìš©ìê°€ ì œê³µí•œ ì •í™•í•œ ì¼€ì´ìŠ¤ë¡œ ë‹¨ê³„ë³„ ë¶„ì„
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from crawler import NaverPlaceCrawler
import requests
import urllib.parse
from bs4 import BeautifulSoup

def analyze_user_case():
    """ì‚¬ìš©ì ì œê³µ ì¼€ì´ìŠ¤ ë¶„ì„"""
    print("ğŸ” ì‚¬ìš©ì ì¼€ì´ìŠ¤ ë¶„ì„")
    print("=" * 50)
    
    # ì‚¬ìš©ì ì œê³µ ì •ë³´
    place_url = "https://m.place.naver.com/restaurant/38758389"
    place_name = "ë§¥ë„ë‚ ë“œìƒì•”DMCì "
    search_keyword = "ì„œìš¸ ìƒì•” ë§›ì§‘"
    
    print(f"ğŸ“ í”Œë ˆì´ìŠ¤ URL: {place_url}")
    print(f"ğŸª í”Œë ˆì´ìŠ¤ëª…: {place_name}")
    print(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keyword}")
    print()
    
    # 1ë‹¨ê³„: URL ê²€ì¦
    print("1ï¸âƒ£ URL íŒŒì‹± ê²€ì¦")
    try:
        from urllib.parse import urlparse
        parsed = urlparse(place_url)
        path_parts = parsed.path.split('/')
        place_id = path_parts[-1] if path_parts else None
        print(f"   âœ… í˜¸ìŠ¤íŠ¸: {parsed.hostname}")
        print(f"   âœ… ê²½ë¡œ: {parsed.path}")
        print(f"   âœ… í”Œë ˆì´ìŠ¤ ID: {place_id}")
    except Exception as e:
        print(f"   âŒ URL íŒŒì‹± ì˜¤ë¥˜: {e}")
    
    # 2ë‹¨ê³„: í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
    print("\n2ï¸âƒ£ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”")
    try:
        crawler = NaverPlaceCrawler()
        search_url = crawler.build_url(search_keyword)
        print(f"   âœ… í¬ë¡¤ëŸ¬ ìƒì„±ë¨")
        print(f"   âœ… ê²€ìƒ‰ URL: {search_url}")
    except Exception as e:
        print(f"   âŒ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return
    
    # 3ë‹¨ê³„: ë„¤íŠ¸ì›Œí¬ ìš”ì²­ í…ŒìŠ¤íŠ¸
    print("\n3ï¸âƒ£ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ í…ŒìŠ¤íŠ¸")
    try:
        session = requests.Session()
        session.headers.update(crawler.headers)
        
        response = session.get(search_url, timeout=10)
        print(f"   âœ… ìƒíƒœ ì½”ë“œ: {response.status_code}")
        print(f"   âœ… ì‘ë‹µ í¬ê¸°: {len(response.text)} bytes")
        print(f"   âœ… Content-Type: {response.headers.get('content-type', 'Unknown')}")
        
        if response.status_code != 200:
            print(f"   âŒ ë¹„ì •ìƒ ìƒíƒœ ì½”ë“œ")
            return
            
    except Exception as e:
        print(f"   âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return
    
    # 4ë‹¨ê³„: HTML êµ¬ì¡° ë¶„ì„
    print("\n4ï¸âƒ£ HTML êµ¬ì¡° ë¶„ì„")
    soup = BeautifulSoup(response.text, "html.parser")
    
    # ê¸°ë³¸ êµ¬ì¡° í™•ì¸
    title = soup.find('title')
    print(f"   ğŸ“„ í˜ì´ì§€ ì œëª©: {title.get_text() if title else 'None'}")
    
    # ìë°”ìŠ¤í¬ë¦½íŠ¸ ë¦¬ë‹¤ì´ë ‰íŠ¸ë‚˜ ì—ëŸ¬ í˜ì´ì§€ í™•ì¸
    if "window.location" in response.text or "redirect" in response.text.lower():
        print("   âš ï¸  ë¦¬ë‹¤ì´ë ‰íŠ¸ ê°ì§€ë¨")
    
    if "error" in response.text.lower() or "ì˜¤ë¥˜" in response.text:
        print("   âš ï¸  ì—ëŸ¬ í˜ì´ì§€ì¼ ìˆ˜ ìˆìŒ")
    
    # 5ë‹¨ê³„: ë¦¬ìŠ¤íŠ¸ ìš”ì†Œ ì°¾ê¸°
    print("\n5ï¸âƒ£ ë¦¬ìŠ¤íŠ¸ ìš”ì†Œ ê²€ìƒ‰")
    
    # í˜„ì¬ í¬ë¡¤ëŸ¬ì˜ ì„ íƒìë“¤ í…ŒìŠ¤íŠ¸
    mobile_selectors = [
        "div.list_place li",
        "ul.list_place li", 
        "li.place_item",
        "div.place_list li",
        "div.search_list li",
        "ul li",  # ê°€ì¥ ì¼ë°˜ì ì¸ ì„ íƒì
    ]
    
    for selector in mobile_selectors:
        try:
            items = soup.select(selector)
            print(f"   ğŸ“‹ {selector:20}: {len(items):3d}ê°œ")
            
            # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” í•­ëª©ë§Œ ì„¸ê¸°
            text_items = [item for item in items if item.get_text().strip()]
            if len(text_items) != len(items):
                print(f"       (í…ìŠ¤íŠ¸ í¬í•¨: {len(text_items)}ê°œ)")
                
        except Exception as e:
            print(f"   âŒ {selector:20}: {e}")
    
    # 6ë‹¨ê³„: ë§¥ë„ë‚ ë“œ í…ìŠ¤íŠ¸ ê²€ìƒ‰
    print("\n6ï¸âƒ£ ëª©í‘œ í”Œë ˆì´ìŠ¤ ê²€ìƒ‰")
    
    # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë§¥ë„ë‚ ë“œ ê²€ìƒ‰
    all_text = soup.get_text()
    if "ë§¥ë„ë‚ ë“œ" in all_text:
        print("   âœ… í˜ì´ì§€ì— 'ë§¥ë„ë‚ ë“œ' í…ìŠ¤íŠ¸ ì¡´ì¬")
        
        # ë§¥ë„ë‚ ë“œê°€ í¬í•¨ëœ ìš”ì†Œë“¤ ì°¾ê¸°
        mcdonalds_elements = soup.find_all(text=lambda text: text and "ë§¥ë„ë‚ ë“œ" in text)
        print(f"   ğŸ“ ë§¥ë„ë‚ ë“œ í¬í•¨ í…ìŠ¤íŠ¸: {len(mcdonalds_elements)}ê°œ")
        
        for i, element in enumerate(mcdonalds_elements[:5]):
            parent = element.parent
            print(f"      {i+1}. '{element.strip()}'")
            print(f"         ë¶€ëª¨: <{parent.name} class='{parent.get('class', [])}'>")
    else:
        print("   âŒ í˜ì´ì§€ì— 'ë§¥ë„ë‚ ë“œ' í…ìŠ¤íŠ¸ ì—†ìŒ")
        print("   ğŸ” ëŒ€ì‹  ë‹¤ë¥¸ ë§›ì§‘ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸:")
        
        # ì¼ë°˜ì ì¸ ìŒì‹ì  í‚¤ì›Œë“œë“¤ ê²€ìƒ‰
        food_keywords = ["ì¹˜í‚¨", "í”¼ì", "ë²„ê±°", "ì¹´í˜", "ì‹ë‹¹", "ìŒì‹ì "]
        for keyword in food_keywords:
            if keyword in all_text:
                count = all_text.count(keyword)
                print(f"      ğŸ“ '{keyword}': {count}ë²ˆ ë°œê²¬")
    
    # 7ë‹¨ê³„: ì‹¤ì œ í¬ë¡¤ëŸ¬ ì‹¤í–‰
    print("\n7ï¸âƒ£ ì‹¤ì œ í¬ë¡¤ëŸ¬ ì‹¤í–‰")
    try:
        result = crawler.search_place_rank(search_keyword, place_name)
        print(f"   ğŸ¯ í¬ë¡¤ë§ ê²°ê³¼:")
        print(f"      ì„±ê³µ: {result['success']}")
        print(f"      ìˆœìœ„: {result['rank']}")
        print(f"      ë©”ì‹œì§€: {result['message']}")
        print(f"      ë°œê²¬ëœ ìƒì  ìˆ˜: {len(result.get('found_shops', []))}")
        
        if result.get('found_shops'):
            print(f"      ë°œê²¬ëœ ìƒì ë“¤:")
            for i, shop in enumerate(result['found_shops'][:10]):
                print(f"         {i+1:2d}. {shop}")
                
    except Exception as e:
        print(f"   âŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
    
    print("\n" + "=" * 50)
    print("ğŸ¯ ë¶„ì„ ì™„ë£Œ!")

def save_debug_html():
    """ë””ë²„ê¹…ìš© HTML íŒŒì¼ ì €ì¥"""
    print("\nğŸ’¾ ë””ë²„ê¹…ìš© HTML ì €ì¥")
    
    crawler = NaverPlaceCrawler()
    search_url = crawler.build_url("ì„œìš¸ ìƒì•” ë§›ì§‘")
    
    try:
        session = requests.Session()
        session.headers.update(crawler.headers)
        response = session.get(search_url, timeout=10)
        
        filename = "debug_seoul_sangam_food.html"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(response.text)
        
        print(f"   âœ… HTML ì €ì¥ë¨: {filename}")
        print(f"   ğŸ” ë¸Œë¼ìš°ì €ì—ì„œ íŒŒì¼ì„ ì—´ì–´ ì‹¤ì œ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        
    except Exception as e:
        print(f"   âŒ HTML ì €ì¥ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    analyze_user_case()
    save_debug_html()